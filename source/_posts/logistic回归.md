title: Logistic回归
date: 2018/11/13
categories:
- 数据处理
- 机器学习
tags:
-  Logistic回归
comments: true
---

1. [Logistic回归](https://github.com/apachecn/MachineLearning/blob/master/docs/5.Logistic%E5%9B%9E%E5%BD%92.md)
    - **描述**：
        - **分类算法**，回归是对一些数据点进行拟合，该拟合过程称为回归，这样的思想是用来做预测的。为什么能用来分类呢？
        - 假设有一个这样的函数，f(z) = 1/(1 + e^(-z)), 显然随着z 增大 f(z)的值逼于1; 随着z的减小，f(z) = 0。 如果把自变量Z的值范围扩大到，如-20 到 20， f(z)就是一条突变的0到1的很陡曲线。
        - 结合机器学习 数据集(含特征和类别)
            ```
            特征1 x0    特征2 x1        类别
            -0.017612	14.053064	     0
            -1.395634	4.662541	     1
            -0.752157	6.538620	     0
            -1.322371	7.152853	     0
            0.423363	11.054677	     0
            
            假设 f(z) 的自变量 z = w0*x0 + w1*x1    (w0,w1 就是接下来要求的!)
            将 z 代入 f(z)，该z使得f(z)产生突变， 即可得到在0 < f(z) < 1 
            规定 f(z) < 0.5, 该数据 归为第一类
            规定 0.5 < f(z), 该数据 归为第二类
            
            求 w0, w1 就是实现这一思想核心！
            ```
    - 相关数学知识：
    1. 梯度
        - 假设 z = f(x, y) 三维图像，类似一座山峰的表面图形，像一顶帽子的图像。
        - **梯度**：假设取山峰上一点 A(x0, y0, z0)，接下你想朝一个最陡峭的方向 往上爬。该最陡峭的方向即为 A 点的 梯度。
        - 梯度求法：
            ```
            z = f(x, y)
            
            a向量 = ( 函数f对x 求偏导， 函数f对y 求偏导)
            
            求在 A(x0, y0, z0) 的梯度为 将x0， y0 带入 a向量
            
            如： z = 2*x^2 + y^2
            
            a = (4x, 2y)
            A的 梯度为 (4x0, 2y0)
            ```
        - **移动步长**：k
            ```
            A1 = (x0, y0) + k * a
            
            这时 沿着梯度方向a (最陡峭方向) 爬一定的长度，
            就到了新的一个点A1， 再按照该方法就可以 到新的点A2
            
            这样一直爬下去A3, A4.....An 就爬到了最高点An
            ```
    - 疑问
        - 求 w0, w1 是利用 迭代，为什么后一次迭代效果比前一次好。
        - 每一次迭代用了梯度，但是不理解，梯度在这里的用处！
        - 其他书籍有的，通过损失函数的概念，来求 w0,w1
    - 运行代码

      ​    


欢迎大家给我留言，提建议，指出错误，一起讨论学习技术的感受！