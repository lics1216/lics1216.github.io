title: 回归和树回归
date: 2018/11/17
categories:
- 数据处理
- 机器学习
tags:
-   回归和树回归
comments: true
---
1. 预测数值型数据：回归
    - **描述**：用于回归，数据集是连续型的，用线性回归找到最佳拟合直线。
    - 算法原理：
        1. 最小二乘法
            ```
            普通线性回归：最小二乘法
            假设拟合 方程为：y = w0 + w1x1 + w2x2 + .... + wnxn
            拟合值和实际值 是有误差的，使得误差最小的情况下，求出系数。
            
            防止误差的正负相互抵消，利用平方的形式求和。
            误差平方和，用向量的形式表达，求其最小值，在导数等于0处求得。
            
            求得相应的公式。
            
            ```
        2. 预测值和真实值的匹配程度，可以通过两个序列的**相关系数**来判断。
        3. **局部加权**线性回归
            - 给每个**数据点赋予权重**
            - 读入数据并创建所需矩阵，之后创建对角权重矩阵 weights
        4. 预测鲍鱼的年龄，对比普通线性回归 和 局部线性回归
        5. **数据的特征比样本点还多**的情况(输入矩阵x, xTx **无法求逆**)
    - 三种处理方法
        1. 岭回归
        2. lasso
        3. 前向逐渐回归
    - 权衡偏差 与 方差
    - **关键特征**：结合预测乐高玩具套装的价格说明，通过观察 岭回归 的回归系数的变化过程(循环10次)，可以得到哪个特征是关键的，当特征有上百个时，该方法十分有效！ 

2. 树回归
    - **描述**：
        - 普通线性回归要拟合所有的样本(局部加权线性回归除外)，有些问题是非线性的，不能使用全集线性模型来拟合。
        - 另外一种方法是，将数据集切分，然后利用线性回归技术建模，比如树回归。
    - **算法原理**：
    1. 切分数据集算法，前者**决策树使用的是 ID3 算法**， 按照特征的不同特征值来确定切分份数，但这样无法处理连续型数据。**回归树利用CART进行二元切分**，小于该特征值归为左子树，大于归为右子树。
    2. **构建回归树**
        - 数据集
            ```
             常数项      特征x0      要预测y
            1.000000	0.067732	3.176513
            1.000000	0.427810	3.816464
            1.000000	0.995731	4.550095
            1.000000	0.738336	4.256571
            1.000000	0.981083	4.560815
            1.000000	0.526171	3.929515
            1.000000	0.378887	3.526170
            1.000000	0.033859	3.156393
            1.000000	0.132791	3.110301
            1.000000	0.138306	3.149813
            ......
            
            假设 图为：
                 |
            2.0  |                   ..    .
                 |                    .     .     ..
            1.5  |                  .     ..
                 |                  .. .
            1.0  |                      .   . .
                 |     .             . .
            0.5  | .. .   .
                 | .. .   .  .
            0.0  |   .   .  .
                 |    . .  .
            -0.5 |     .
                 |
            -1.0 |_______________________________________ 
            -0.2  0.0  0.2  0.4  0.6  0.8  1.0   1.2
            ```
        - **寻找最佳切分特征，特征值**，chooseBestSplit()
            ```
            对每个特征：
                对每个特征值：
                    将数据切分成两份
                    计算误差，
                    如果当前误差小于最小误差，更新误差、最优特征值
            返回最佳切分 特征、特征值
            
            其中误差计算，采用 总方差 的形式。
            对于数据集， 首先计算所有数据的均值，然后计算每条数据的值
            到均值的差值的平方，求和。 即，为总方差。
            ```
        - **构建回归树，递归过程**
        
            ```
            当交给一 个待切分 的子集给 chooseBestSlit()
            需要考虑各种 切分 的情况
            
            1. 如果所有 值相等则退出
            2. 如果切分后误差减小不大，则退出，设置停止条件 tols
            3. 如果切分出来的数据集很小，则退出
            
            ```
        - 结果：
        ```
                 |
            2.0  |                   
                 |                           .......
            1.5  |                  
                 |                      .......
            1.0  |                      
                 |                  .....
            0.5  | 
                 |            ........
            0.0  |   
                 |        ........
            -0.5 |    ......
                 |
            -1.0 |_______________________________________ 
                -0.2  0.0  0.2  0.4  0.6  0.8  1.0   1.2
        ```
        - 叶节点(就是不在进行切分的数据集)，设置为一个常数，即，全部数据的均值。
    - 树剪枝
        - 描述：如果一个树的节点过多，可能是过拟合了。
        - 预剪枝：手动修改停止条件tols 对误差的数量级敏感度
        - 后剪枝：合并叶节点，如果合并降低误差，就将叶节点合并。
    
    - **模型树**
    1. 把叶节点由 常数 设置为分段线性函数
    2. 确定不再切分子集后，用普通线性回归，计算出该 回归系数
    3. 结果
        ```
                     |
                2.0  |                .   
                     |               .           
                1.5  |              .    
                     |             .         
                1.0  |            .          
                     |           .      
                0.5  |          .
                     |         .   
                0.0  |        .     
                     |       .
                -0.5 |  .....
                     |
                -1.0 |_______________________________________ 
                    -0.2  0.0  0.2  0.4  0.6  0.8  1.0   1.2   
        ```
    - **普通线性回归 VS 树回归 VS 模型树**
        - 分别计算由以上算法得来的** 预测值yHat**， 和目标值**实际值y**的 **相关系数**
        - 利用 NumPy `corrcoef(yHat, y)` 来计算相关系数
        
    - **GUI方便算法调参**
        - 利用 python包 Tkinter 创建 GUI
        - 集成 Matplotlib 和 Tkinter

          


欢迎大家给我留言，提建议，指出错误，一起讨论学习技术的感受！